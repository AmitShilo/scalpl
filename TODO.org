
* Thesis Statement
The software consists of three services:
** Order Placement Engine
*** performs
offer placement in a market according to guidelines provided by an investor
*** receives guidelines as
a set of numerical parameters representing
**** risk tolerances,
**** hedging requirements,
**** profit targets
**** a market sentiment, defined as
***** interpretations of past market events
***** policies for interpreting current market events
***** expectations regarding future market events
** Market Observer
tool for evaluating statistical queries on market events, past and present
** Network                                                             :WIP:
membership in a profit-sharing arbitrage cooperative, operating as a decentralized neural network.
* Exchange modularity
** Need to distinguish between tracking a market and participating in it
** Participation should be mediated by rate gates
* Account
** Contents:
*** exchange / gate object
**** executes commands
**** obeys rate limit
*** balance manager
**** tracks asset balances
**** handles hedging requirements and target exposures
**** reports asset balances
**** calculates liquidity allocation plan
*** offer manager
**** tracks open offers
**** routes limit orders and cancellations to the exchange
**** performs on-demand analysis on offer distributions
**** limit orders placement according to priority (ie "best" price)
*** command executor
**** translates limit orders and cancellations into API calls
**** filters out "EOrder:Insufficient funds" errors
(they'll get placed again next round)
*** offer execution tracker
**** downloads offer execution backlog
**** tracks execution of my offers
**** performs on-demand analysis on execution stream
***** emvwap, duplex and directional
***** order flow optimization
***** update offer handler
* Trades-Tracker
** Level 2 order book!
Think later of ways to do this efficiently, right now we're just interested in
the high-level so we can express statistical arbitrage rules
** Trade Direction
*** Some exchanges provide this information in the trades data
*** For exchanges that don't, we use a classifier:
**** continually tracks best few offers on the book
**** Was the last trade >= the lowest ask? -> buyer initiative
**** Was the last trade <= the highest bid? -> seller initiative
* Dumbot
** Resilience
*** Definition
How large a buy or sell we want to suvive without getting "run over"
*** Old definition - included for reference
Our buy resilience is 4BTC, we have 0.5BTC to sell, and the ask-side order book
looks like:
|     Price |     Volume |      Depth | Our Liquidity |
|-----------+------------+------------+---------------|
| 350.00000 | 0.05000000 | 0.05000000 |               |
| 350.02859 | 0.10000000 | 0.15000000 |               |
| 350.18932 | 0.87382719 |  1.0238272 |               |
| 350.71930 | 0.18990000 |  1.2137272 |               |
| 350.99999 | 0.15000000 |  1.3637272 |               |
| 351.00000 | 2.00000000 |  3.3637272 |               |
| 351.59920 | 0.39996200 |  3.7636892 |               |
We'd thus want to spread out our 0.5BTC between the best possible ask, and just
before the last ask with a depth less than our resilience. It should spread out
the orders proportionally to the depth necessary to reach each one -- thus, we
scale our available liquidity by the VOLUME AT each order,
beginning from the minimal order size (say, 0.001 BTC), and up as high as
possible. The overall goal is not to change the shape of the order book, just
increase its liquidity.
*** Resilience is now more complex
We should at least have separate resilience for each side of the order book, if
not even distinct levels of funds, each bound at different resilience levels.
** Inputs:
(for just one side of the algorithm)
*** Order book
*** Resilience
*** Funds
